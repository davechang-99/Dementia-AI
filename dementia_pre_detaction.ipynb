{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davechang-99/Dementia-AI/blob/main/dementia_pre_detaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnnH260PQvbA"
      },
      "source": [
        "셀 1: 환경 설정 및 런타임 재시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DurFXoppPTcp",
        "outputId": "5475e070-ead0-49cc-8cbc-f71be8fb16f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.1\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2025.7.14)\n",
            "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.9.0 requires torch>=2.0.0, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "peft 0.16.0 requires torch>=1.13.0, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ViT 오류 방지를 위한 transformers 버전 고정 및 torch, numpy, librosa 등 주요 패키지의 호환 버전 재설치 후 런타임 재시작\n",
        "!pip install transformers==4.40.1\n",
        "!pip uninstall -y numpy matplotlib torch torchvision torchaudio librosa > /dev/null\n",
        "!pip install numpy==1.26.4 matplotlib==3.8.0 librosa==0.10.1 --quiet\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# 설치된 패키지 버전 확인\n",
        "import numpy, torch, torchvision, librosa\n",
        "print(\"✅ numpy:\", numpy.__version__)\n",
        "print(\"✅ torch:\", torch.__version__)\n",
        "print(\"✅ torchvision:\", torchvision.__version__)\n",
        "print(\"✅ librosa:\", librosa.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KrK-baMQ6gx"
      },
      "source": [
        "셀 2: GPU/CPU 장치 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU8OOkBsPZt9",
        "outputId": "703aa536-ec70-4010-ed5a-2a6103a81246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEVICE] Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# PyTorch를 이용하여 CUDA가 가능한 경우 GPU, 아니면 CPU 장치를 자동으로 선택하여 연산 환경 설정\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[DEVICE] Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aN2UucGSWY3"
      },
      "source": [
        "셀 3: Google Drive 마운트 및 데이터셋 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw8vg6a8PZw2",
        "outputId": "27a09718-3b82-4f75-efd4-c9e04a1a65bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "📂 데이터셋 위치: /content/drive/MyDrive/DATASET_A\n"
          ]
        }
      ],
      "source": [
        "# ✅ Google Drive 마운트\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ 데이터셋 경로 설정\n",
        "dataset_path = Path(\"/content/drive/MyDrive/DATASET_A\")\n",
        "print(\"📂 데이터셋 위치:\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeJsVmL2RAC4"
      },
      "source": [
        "셀 4: 폴더 구조 출력 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1iB_r8fPZzs",
        "outputId": "770ef8df-d652-49a9-b364-9269cbc0d136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 폴더 구조만 출력:\n",
            "📁 DATASET_A/\n",
            "    📁 Dementia/\n",
            "        📁 train/\n",
            "        📁 val/\n",
            "    📁 Normal/\n",
            "        📁 train/\n",
            "        📁 val/\n"
          ]
        }
      ],
      "source": [
        "# ✅ 하위 폴더 구조만 출력하는 함수 정의\n",
        "def print_folder_tree(path: Path, indent: str = \"\"):\n",
        "    if path.is_dir():\n",
        "        print(f\"{indent}📁 {path.name}/\")\n",
        "        for child in sorted(path.iterdir()):\n",
        "            if child.is_dir():\n",
        "                print_folder_tree(child, indent + \"    \")\n",
        "\n",
        "# ✅ 폴더 구조 출력\n",
        "print(\"✅ 폴더 구조만 출력:\")\n",
        "print_folder_tree(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgtnavQ_RDyI"
      },
      "source": [
        "셀 5: Mel-spectrogram 이미지 변환 및 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsO3Knn1PdTM",
        "outputId": "4d4ccc37-ced7-413d-da19-19b5f2f6ec13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎧 변환 중: Normal/train → 총 200개 파일\n",
            "🎧 변환 중: Normal/val → 총 99개 파일\n",
            "🎧 변환 중: Dementia/train → 총 240개 파일\n",
            "🎧 변환 중: Dementia/val → 총 60개 파일\n",
            "✅ 모든 Mel-spectrogram 이미지 저장 완료\n"
          ]
        }
      ],
      "source": [
        "# ✅ 라이브러리 불러오기\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "# ✅ 원본 음성 데이터 경로\n",
        "dataset_path = Path(\"/content/drive/MyDrive/DATASET_A\")\n",
        "\n",
        "# ✅ Mel-spectrogram 저장 경로\n",
        "mel_output_dir = Path(\"/content/MEL_DATASET\")\n",
        "\n",
        "# ✅ 클래스 및 분할 리스트\n",
        "classes = [\"Normal\", \"Dementia\"]\n",
        "splits = [\"train\", \"val\"]\n",
        "\n",
        "# ✅ 동일한 폴더 구조 생성\n",
        "for cls in classes:\n",
        "    for split in splits:\n",
        "        target_dir = mel_output_dir / cls / split\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ✅ Mel-spectrogram 저장 함수 정의\n",
        "def save_mel(wav_path, save_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(wav_path, sr=16000)\n",
        "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        plt.figure(figsize=(2.24, 2.24), dpi=100)\n",
        "        librosa.display.specshow(mel_db, sr=sr, cmap='viridis')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 에러 발생: {wav_path} → {e}\")\n",
        "\n",
        "# ✅ 변환 실행\n",
        "for cls in classes:\n",
        "    for split in splits:\n",
        "        input_dir = dataset_path / cls / split\n",
        "        output_dir = mel_output_dir / cls / split\n",
        "\n",
        "        wav_files = sorted(input_dir.glob(\"*.wav\"))\n",
        "        print(f\"🎧 변환 중: {cls}/{split} → 총 {len(wav_files)}개 파일\")\n",
        "\n",
        "        for wav_path in wav_files:\n",
        "            output_path = output_dir / (wav_path.stem + \".png\")\n",
        "            save_mel(wav_path, output_path)\n",
        "\n",
        "print(\"✅ 모든 Mel-spectrogram 이미지 저장 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIlLv7TsRGzu"
      },
      "source": [
        "셀 6: Dataset 클래스, 전처리 및 DataLoader 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVE__AtBYl6O"
      },
      "outputs": [],
      "source": [
        "# ✅ 라이브러리 임포트\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# ✅ 가우시안 노이즈 추가를 위한 커스텀 변환 클래스 정의\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "# ✅ Mel-spectrogram 이미지 Dataset 클래스\n",
        "class MelImageDataset(Dataset):\n",
        "    def __init__(self, base_dir, split=\"train\", transform=None):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        self.classes = [\"Normal\", \"Dementia\"]\n",
        "\n",
        "        for label, cls in enumerate(self.classes):\n",
        "            cls_path = Path(base_dir) / cls / split\n",
        "            img_paths = sorted(cls_path.glob(\"*.png\"))\n",
        "            self.samples.extend(img_paths)\n",
        "            self.labels.extend([label] * len(img_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.samples[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# ✅ 훈련용 이미지 전처리 (데이터 증강 포함)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0., std=0.01)\n",
        "])\n",
        "\n",
        "# ✅ 검증용 이미지 전처리 (데이터 증강 없음)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ✅ Dataset 및 DataLoader 정의\n",
        "base_path = \"/content/MEL_DATASET\"\n",
        "train_data = MelImageDataset(base_path, split=\"train\", transform=train_transform)\n",
        "val_data = MelImageDataset(base_path, split=\"val\", transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Zu3vmsRLoZ"
      },
      "source": [
        "셀 7: CNN 및 ViT 모델 클래스 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5TvLDXPdY0"
      },
      "outputs": [],
      "source": [
        "# ✅ 라이브러리 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "\n",
        "# CNN 모델 클래스 정의\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ViT 모델 클래스 정의\n",
        "class ViTClassifier(nn.Module):\n",
        "    # processor를 __init__에서 한 번만 초기화하여 효율성 개선\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "        self.fc = nn.Linear(self.vit.config.hidden_size, 2)\n",
        "        self.processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "    # forward 함수 내에서 프로세서를 재사용\n",
        "    def forward(self, x):\n",
        "        # 텐서 이미지를 PIL 이미지 리스트로 변환\n",
        "        pil_images = [transforms.ToPILImage()(img.cpu()) for img in x]\n",
        "        # 프로세서를 사용하여 입력 데이터 준비\n",
        "        inputs = self.processor(images=pil_images, return_tensors=\"pt\").to(x.device)\n",
        "        outputs = self.vit(**inputs)\n",
        "        return self.fc(outputs.last_hidden_state[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P5DpZjlRPYl"
      },
      "source": [
        "셀 8: Adaboost용 데이터 로딩 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TStko4Mvai8T",
        "outputId": "d8051a69-dce9-4f4d-beb9-10ebad36925d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터 크기: (440, 4096), 레이블: (440,)\n",
            "검증 데이터 크기: (159, 4096), 레이블: (159,)\n",
            "\n",
            "✅ 훈련 데이터와 검증 데이터 간에 중복 파일이 없습니다. Adaboost 모델의 성능을 재확인하세요.\n"
          ]
        }
      ],
      "source": [
        "# ✅ 라이브러리 임포트\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import os # os 라이브러리 추가\n",
        "\n",
        "# Adaboost용 이미지 로딩 함수 (폴더 구조 반영)\n",
        "def load_images_and_labels(base_dir, split=\"train\", size=(64, 64)):\n",
        "    X, y = [], []\n",
        "    classes = [\"Normal\", \"Dementia\"]\n",
        "\n",
        "    for label, cls in enumerate(classes):\n",
        "        cls_path = Path(base_dir) / cls / split\n",
        "        img_paths = sorted(cls_path.glob(\"*.png\"))\n",
        "\n",
        "        for path in img_paths:\n",
        "            img = Image.open(path).convert(\"L\").resize(size)\n",
        "            X.append(np.array(img).flatten())\n",
        "            y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ✅ base_dir는 MEL_DATASET 폴더의 루트 경로\n",
        "base_dir = \"/content/MEL_DATASET\"\n",
        "\n",
        "# 훈련용 이미지 벡터 로딩\n",
        "X_train, y_train = load_images_and_labels(base_dir, split=\"train\")\n",
        "\n",
        "# 검증용 이미지 벡터 로딩\n",
        "X_val, y_val = load_images_and_labels(base_dir, split=\"val\")\n",
        "\n",
        "print(f\"훈련 데이터 크기: {X_train.shape}, 레이블: {y_train.shape}\")\n",
        "print(f\"검증 데이터 크기: {X_val.shape}, 레이블: {y_val.shape}\")\n",
        "\n",
        "# ✅ 데이터셋 중복 확인 코드 추가\n",
        "train_filenames = {os.path.basename(p) for p in (Path(base_dir) / \"Normal\" / \"train\").glob(\"*.png\")}\n",
        "train_filenames.update({os.path.basename(p) for p in (Path(base_dir) / \"Dementia\" / \"train\").glob(\"*.png\")})\n",
        "\n",
        "val_filenames = {os.path.basename(p) for p in (Path(base_dir) / \"Normal\" / \"val\").glob(\"*.png\")}\n",
        "val_filenames.update({os.path.basename(p) for p in (Path(base_dir) / \"Dementia\" / \"val\").glob(\"*.png\")})\n",
        "\n",
        "# 교집합을 찾아 중복 파일 확인\n",
        "overlap = train_filenames.intersection(val_filenames)\n",
        "\n",
        "if len(overlap) > 0:\n",
        "    print(f\"\\n❌ 경고: 훈련 데이터와 검증 데이터에 중복 파일이 {len(overlap)}개 발견되었습니다.\")\n",
        "    print(f\"중복 파일 예시: {list(overlap)[:5]}\")\n",
        "    print(\"→ Adaboost 모델의 완벽한 성능은 이 중복 때문일 가능성이 높습니다.\")\n",
        "else:\n",
        "    print(\"\\n✅ 훈련 데이터와 검증 데이터 간에 중복 파일이 없습니다. Adaboost 모델의 성능을 재확인하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7rqcfbGxYVS"
      },
      "source": [
        "셀 9: 모델 학습(Random Forest) 및 최종 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NLg2mQ5is4o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier # RandomForestClassifier 추가\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ 학습 함수\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            preds = model(images)\n",
        "            loss = criterion(preds, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# ✅ 하이퍼파라미터 튜닝 함수\n",
        "def tune_cnn_hyperparameters(train_data, val_data):\n",
        "    learning_rates = [1e-3, 1e-4]\n",
        "    batch_sizes = [16, 32]\n",
        "    epochs = 5\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    print(\"--- CNN 하이퍼파라미터 튜닝 시작 ---\")\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"\\n[튜닝] LR: {lr}, 배치 크기: {bs}\")\n",
        "            train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(val_data, batch_size=bs)\n",
        "\n",
        "            cnn_model = CNNClassifier()\n",
        "            accuracy = train_model(cnn_model, train_loader, val_loader, epochs=epochs, lr=lr)\n",
        "\n",
        "            print(f\"✅ 검증 정확도: {accuracy:.2f}%\")\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {'lr': lr, 'batch_size': bs}\n",
        "\n",
        "    print(\"\\n--- 튜닝 결과 ---\")\n",
        "    print(f\"✅ 최적 하이퍼파라미터: {best_params}\")\n",
        "    print(f\"✅ 최고 검증 정확도: {best_accuracy:.2f}%\")\n",
        "    return best_params\n",
        "\n",
        "# -----------------\n",
        "# 1. 최적화된 하이퍼파라미터로 CNN 모델 재학습 및 평가\n",
        "# -----------------\n",
        "best_cnn_params = tune_cnn_hyperparameters(train_data, val_data)\n",
        "epochs = 10\n",
        "\n",
        "print(\"\\n--- 최적화된 하이퍼파라미터로 CNN 모델 학습 ---\")\n",
        "train_loader_cnn = DataLoader(train_data, batch_size=best_cnn_params['batch_size'], shuffle=True)\n",
        "val_loader_cnn = DataLoader(val_data, batch_size=best_cnn_params['batch_size'])\n",
        "cnn_model = CNNClassifier()\n",
        "train_model(cnn_model, train_loader_cnn, val_loader_cnn, epochs=epochs, lr=best_cnn_params['lr'])\n",
        "\n",
        "# CNN 모델 예측 (확률 포함)\n",
        "cnn_model.eval()\n",
        "cnn_preds_proba = []\n",
        "cnn_preds = []\n",
        "cnn_true = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader_cnn:\n",
        "        images = images.to(device)\n",
        "        outputs = cnn_model(images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        cnn_preds_proba.extend(probabilities.cpu().numpy()[:, 1])\n",
        "        cnn_preds.extend(predicted.cpu().numpy())\n",
        "        cnn_true.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 2. ViT 모델 학습 및 평가\n",
        "# -----------------\n",
        "print(\"\\n--- ViT 모델 학습 ---\")\n",
        "train_loader_vit = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader_vit = DataLoader(val_data, batch_size=16)\n",
        "vit_model = ViTClassifier()\n",
        "train_model(vit_model, train_loader_vit, val_loader_vit, epochs=5)\n",
        "\n",
        "# ViT 모델 예측 (확률 포함)\n",
        "vit_model.eval()\n",
        "vit_preds_proba = []\n",
        "vit_preds = []\n",
        "vit_true = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader_vit:\n",
        "        images = images.to(device)\n",
        "        outputs = vit_model(images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        vit_preds_proba.extend(probabilities.cpu().numpy()[:, 1])\n",
        "        vit_preds.extend(predicted.cpu().numpy())\n",
        "        vit_true.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 3. Adaboost를 대체할 RandomForest 모델 학습 및 평가\n",
        "# -----------------\n",
        "print(\"\\n--- RandomForest 모델 학습 ---\")\n",
        "# n_estimators: 트리의 개수, max_depth: 트리의 최대 깊이\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# RandomForest 모델 훈련 데이터 성능 확인\n",
        "y_pred_rf_train = rf_model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_pred_rf_train) * 100\n",
        "print(f\"\\n--- RandomForest 모델 훈련 데이터 정확도: {train_accuracy:.2f}% ---\")\n",
        "\n",
        "# RandomForest 모델 예측 (확률 포함)\n",
        "y_pred_rf_proba = rf_model.predict_proba(X_val)[:, 1]\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "y_true_rf = y_val\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 4. 최종 결과 시각화 및 성능 비교 표\n",
        "# -----------------\n",
        "def plot_results(y_true, y_pred, y_proba, model_name):\n",
        "    print(f\"\\n--- {model_name} 결과 ---\")\n",
        "    report = classification_report(y_true, y_pred, target_names=[\"Normal\", \"Dementia\"], output_dict=True)\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Dementia\"]))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Dementia\"], yticklabels=[\"Normal\", \"Dementia\"])\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"✅ {model_name} AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return report, roc_auc\n",
        "\n",
        "# 각 모델별 결과 시각화 및 성능 지표 수집\n",
        "cnn_report, cnn_auc = plot_results(cnn_true, cnn_preds, cnn_preds_proba, \"CNN\")\n",
        "vit_report, vit_auc = plot_results(vit_true, vit_preds, vit_preds_proba, \"ViT\")\n",
        "rf_report, rf_auc = plot_results(y_true_rf, y_pred_rf, y_pred_rf_proba, \"RandomForest\")\n",
        "\n",
        "\n",
        "# ✅ 성능 비교 표 생성 및 출력\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['CNN', 'ViT', 'RandomForest'],\n",
        "    'Accuracy': [cnn_report['accuracy'], vit_report['accuracy'], rf_report['accuracy']],\n",
        "    'Precision': [cnn_report['weighted avg']['precision'], vit_report['weighted avg']['precision'], rf_report['weighted avg']['precision']],\n",
        "    'Recall': [cnn_report['weighted avg']['recall'], vit_report['weighted avg']['recall'], rf_report['weighted avg']['recall']],\n",
        "    'F1-Score': [cnn_report['weighted avg']['f1-score'], vit_report['weighted avg']['f1-score'], rf_report['weighted avg']['f1-score']],\n",
        "    'AUC': [cnn_auc, vit_auc, rf_auc]\n",
        "}).set_index('Model')\n",
        "\n",
        "print(\"\\n--- 모델 성능 비교표 ---\")\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}